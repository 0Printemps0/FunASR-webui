# coding=utf-8

import os
import gradio as gr
import numpy as np
import torch
import torchaudio
from funasr import AutoModel
import gc


# MODELSCOPE_CACHE = 

# åŠ è½½FunASRæ¨¡å‹
def load_model(model_inputs):
	model_abbr = {"çƒ­è¯æ¨¡å‹": "iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch", "æƒ…æ„Ÿæ¨¡å‹": "iic/SenseVoiceSmall"}
	model_inputs = "çƒ­è¯æ¨¡å‹" if len(model_inputs) < 1 else model_inputs
	model_path = model_abbr[model_inputs]
	model = AutoModel(
		model=model_path,  
		vad_model="fsmn-vad", 
		vad_kwargs={"max_single_segment_time": 30000},
		punc_model="ct-punc", 
		spk_model="cam++",
		device="cuda:0",
		disable_update=True,
	)
	print(f"åŠ è½½{model_inputs}æˆåŠŸ")
	return model

# æ¨¡å‹æ¨ç†
def model_inference(input_wav, language, fs=16000):
	gc.collect()
	torch.cuda.empty_cache()
	torch.cuda.ipc_collect()
	language_abbr = {"auto": "auto", "zh": "zh", "en": "en", "yue": "yue", "ja": "ja", "ko": "ko", "nospeech": "nospeech"}
	language = "auto" if len(language) < 1 else language
	selected_language = language_abbr[language]
	if isinstance(input_wav, tuple):
		fs, input_wav = input_wav
		input_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max
		if len(input_wav.shape) > 1:
			input_wav = input_wav.mean(-1)
		if fs != 16000:
			print(f"audio_fs: {fs}")
			resampler = torchaudio.transforms.Resample(fs, 16000)
			input_wav_t = torch.from_numpy(input_wav).to(torch.float32)
			input_wav = resampler(input_wav_t[None, :])[0, :].numpy()
	
	merge_vad = True 
	print(f"language: {language}, merge_vad: {merge_vad}")
	text = model.generate(
		input=input_wav,
		cache={},
		language=selected_language,
		use_itn=True,
		batch_size_s=60, 
		merge_vad=merge_vad,
		merge_length_s=15,
		sentence_timestamp=True,
		hotword='å¥½å“¥å“¥',
	)
	
	print(text) #æ§åˆ¶å°å®Œæ•´è¾“å‡º
	text = text[0]["text"] #é¡µé¢åªè¾“å‡ºæ–‡å­—éƒ¨åˆ†
	
	return text



html_content = """
<div>
	<h2 style="font-size: 22px;margin-left: 0px;text-align: center;">FunASRåº”ç”¨ç¨‹åº FunASR-webui</h2>
</div>
<div style="text-align: center; font-size: 15px; font-weight: bold; color: red; margin-bottom: 20px;">
    âš ï¸ è¯¥æ¼”ç¤ºä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œä½“éªŒä½¿ç”¨ã€‚
</div>
<div style="text-align: center;">
    ä¸€é”®åŒ…åˆ¶ä½œ by åå­—é±¼|
    <a href="https://space.bilibili.com/893892">ğŸŒ Bilibili</a> 
</div>
"""


def launch():
	global model
	model = load_model("çƒ­è¯æ¨¡å‹")
	with gr.Blocks(theme=gr.themes.Soft()) as demo:
		# gr.Markdown(description)
		gr.HTML(html_content)
		with gr.Row():
			with gr.Column():
				audio_inputs = gr.Audio(label="ä¸Šä¼ éŸ³é¢‘æˆ–ä½¿ç”¨éº¦å…‹é£")
				
				with gr.Accordion(label="é…ç½®"):
					model_inputs = gr.Dropdown(label="æ¨¡å‹", choices=["çƒ­è¯æ¨¡å‹", "æƒ…æ„Ÿæ¨¡å‹"], value="çƒ­è¯æ¨¡å‹")
					language_inputs = gr.Dropdown(label="è¯­è¨€", choices=["auto", "zh", "en", "yue", "ja", "ko", "nospeech"], value="auto")
				fn_button = gr.Button("å¼€å§‹", variant="primary")
			text_outputs = gr.Textbox(label="ç»“æœ")
		model_inputs.change(load_model, inputs=model_inputs, outputs=model)
		fn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)

	demo.launch(inbrowser=True, share=False)


if __name__ == "__main__":
	launch()


